{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster  Cantidad de     Porcentaje de   Principales palabras clave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>palabras clave  palabras clave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1     105             15,9 %          maximum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>photo-voltaic  system,  differential   evoluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>13    17              2,6 %           pem   fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>optimisation, particles-size  distributions,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>battery    management,    hydrogen     product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>system-identification.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster  Cantidad de     Porcentaje de   Principales palabras clave\n",
       "0                      palabras clave  palabras clave                 \n",
       "1                                                                     \n",
       "2   ----------------------------------------------...                 \n",
       "3   1     105             15,9 %          maximum ...                 \n",
       "4   photo-voltaic  system,  differential   evoluti...                 \n",
       "..                                                ...                 \n",
       "62  13    17              2,6 %           pem   fu...                 \n",
       "63  optimisation, particles-size  distributions,  ...                 \n",
       "64  battery    management,    hydrogen     product...                 \n",
       "65                             system-identification.                 \n",
       "66                                                                    \n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Divide la línea en columnas\n",
    "        columns = line.strip().split(\"\\t\")  # Ajusta según el formato real del archivo\n",
    "        data.append(columns)\n",
    "\n",
    "# Crea el DataFrame manualmente\n",
    "df = pd.DataFrame(data[1:], columns=data[0]) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(line)\n",
    "try:\n",
    "    # Intentar convertir los primeros caracteres en un número\n",
    "    cluster_num = int(data[9].strip()[:3])\n",
    "    # Si tenemos palabras clave acumuladas, guardarlas antes de continuar\n",
    "    print(\"melos\")\n",
    "except ValueError:\n",
    "    # Si no es un número, acumular palabras clave\n",
    "    print(data[9].strip()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     palabras_clave\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(palabras_clave_temp)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Crear el DataFrame\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCantidad de palabras clave\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcantidad_palabras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPorcentaje de palabras clave\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mporcentajes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrincipales palabras clave\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalabras_clave\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Mostrar el DataFrame\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32mc:\\Users\\usuario\\OneDrive\\Escritorio\\Posgrado\\Descriptiva\\2024-2-PRE-03-ingestion-de-texto-plano-dzaptaca\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\usuario\\OneDrive\\Escritorio\\Posgrado\\Descriptiva\\2024-2-PRE-03-ingestion-de-texto-plano-dzaptaca\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\usuario\\OneDrive\\Escritorio\\Posgrado\\Descriptiva\\2024-2-PRE-03-ingestion-de-texto-plano-dzaptaca\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\usuario\\OneDrive\\Escritorio\\Posgrado\\Descriptiva\\2024-2-PRE-03-ingestion-de-texto-plano-dzaptaca\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de entrada\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "# Listas para almacenar los datos procesados\n",
    "clusters = []\n",
    "cantidad_palabras = []\n",
    "porcentajes = []\n",
    "palabras_clave = []\n",
    "\n",
    "# Abrir y procesar el archivo\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Procesar cada línea del archivo\n",
    "for i, line in enumerate(lines):\n",
    "    # Omitir encabezados y separadores\n",
    "    if i < 4 or line.startswith('-'):\n",
    "        continue\n",
    "    \n",
    "    # Si la línea contiene un nuevo cluster\n",
    "    if line[:3].strip().isdigit():\n",
    "        # Si ya hay datos acumulados, guardarlos\n",
    "        if len(clusters) > 0 and palabras_clave_temp:\n",
    "            palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "        # Reiniciar palabras clave temporales y extraer datos de la nueva línea\n",
    "        palabras_clave_temp = []\n",
    "        parts = line.split()\n",
    "        clusters.append(int(parts[0]))\n",
    "        cantidad_palabras.append(int(parts[1]))\n",
    "        porcentajes.append(parts[2])\n",
    "        palabras_clave_temp.append(\" \".join(parts[3:]))\n",
    "    else:\n",
    "        # Continuar acumulando palabras clave\n",
    "        palabras_clave_temp.append(line.strip())\n",
    "\n",
    "# Añadir las últimas palabras clave acumuladas\n",
    "if palabras_clave_temp:\n",
    "    palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Cluster\": clusters,\n",
    "    \"Cantidad de palabras clave\": cantidad_palabras,\n",
    "    \"Porcentaje de palabras clave\": porcentajes,\n",
    "    \"Principales palabras clave\": palabras_clave\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df)\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV (opcional)\n",
    "df.to_csv(\"clusters_report.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 & 0 & 0 & 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de entrada\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "# Listas para almacenar los datos procesados\n",
    "clusters = []\n",
    "cantidad_palabras = []\n",
    "porcentajes = []\n",
    "palabras_clave = []\n",
    "\n",
    "# Abrir y procesar el archivo\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Procesar cada línea del archivo\n",
    "palabras_clave_temp = []\n",
    "for i, line in enumerate(lines):\n",
    "    # Omitir encabezados y separadores\n",
    "    if i < 4 or line.startswith('-'):\n",
    "        continue\n",
    "    \n",
    "    # Si la línea contiene un nuevo cluster\n",
    "    if line[:3].strip().isdigit():\n",
    "        # Si ya hay datos acumulados, guardarlos\n",
    "        if palabras_clave_temp:\n",
    "            palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "            palabras_clave_temp = []\n",
    "        # Extraer datos del nuevo cluster\n",
    "        parts = lines[i].split()\n",
    "        clusters.append(int(parts[0]))\n",
    "        cantidad_palabras.append(int(parts[1]))\n",
    "        porcentajes.append(parts[2])\n",
    "        palabras_clave_temp.append(\" \".join(parts[3:]))\n",
    "    else:\n",
    "        # Continuar acumulando palabras clave\n",
    "        palabras_clave_temp.append(line.strip())\n",
    "\n",
    "# Añadir las últimas palabras clave acumuladas\n",
    "if palabras_clave_temp:\n",
    "    palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "# Verificar que todas las listas tengan la misma longitud\n",
    "##assert len(clusters) == len(cantidad_palabras) == len(porcentajes) == len(palabras_clave), \\\n",
    "    ##\"Las listas no tienen la misma longitud.\"\n",
    "\n",
    "# Crear el DataFrame\n",
    "##df = pd.DataFrame({\n",
    "    ##\"Cluster\": clusters,\n",
    "    ##\"Cantidad de palabras clave\": cantidad_palabras,\n",
    "    ##\"Porcentaje de palabras clave\": porcentajes,\n",
    "    ##\"Principales palabras clave\": palabras_clave\n",
    "##})\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print( str(len(clusters))+ \" & \"+ str(len(cantidad_palabras))+ \" & \"+ str(len(porcentajes))+ \" & \" +str(len(palabras_clave)))\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV (opcional)\n",
    "#df.to_csv(\"clusters_report.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "szs\n"
     ]
    }
   ],
   "source": [
    "palabras_temp=[\"hola\"]\n",
    "if palabras_temp:\n",
    "    print(\"szs\")\n",
    "else:\n",
    "    print(\"nokas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes inconsistentes: clusters=0, cantidad_palabras=0, porcentajes=0, palabras_clave=1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Las listas no tienen la misma longitud.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 53\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(clusters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(cantidad_palabras) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(porcentajes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(palabras_clave)):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitudes inconsistentes: clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clusters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcantidad_palabras=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cantidad_palabras)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mporcentajes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(porcentajes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpalabras_clave=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(palabras_clave)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLas listas no tienen la misma longitud.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Crear el DataFrame\u001b[39;00m\n\u001b[0;32m     56\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m\"\u001b[39m: clusters,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCantidad de palabras clave\u001b[39m\u001b[38;5;124m\"\u001b[39m: cantidad_palabras,\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPorcentaje de palabras clave\u001b[39m\u001b[38;5;124m\"\u001b[39m: porcentajes,\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrincipales palabras clave\u001b[39m\u001b[38;5;124m\"\u001b[39m: palabras_clave\n\u001b[0;32m     61\u001b[0m })\n",
      "\u001b[1;31mValueError\u001b[0m: Las listas no tienen la misma longitud."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de entrada\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "# Listas para almacenar los datos procesados\n",
    "clusters = []\n",
    "cantidad_palabras = []\n",
    "porcentajes = []\n",
    "palabras_clave = []\n",
    "\n",
    "# Leer y procesar el archivo\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Variables temporales\n",
    "palabras_clave_temp = []\n",
    "\n",
    "# Procesar cada línea del archivo\n",
    "for i, line in enumerate(lines):\n",
    "    # Omitir encabezados y separadores\n",
    "    if i < 4 or line.startswith('-'):\n",
    "        continue\n",
    "    \n",
    "    # Detectar líneas que inician un nuevo cluster\n",
    "    if line[:3].strip().isdigit():\n",
    "        # Si hay palabras clave acumuladas, guardarlas en la lista principal\n",
    "        if palabras_clave_temp:\n",
    "            palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "            palabras_clave_temp = []\n",
    "        \n",
    "        # Extraer datos del cluster actual\n",
    "        parts = line.split(maxsplit=3)\n",
    "        clusters.append(int(parts[0]))  # Número del cluster\n",
    "        cantidad_palabras.append(int(parts[1]))  # Cantidad de palabras clave\n",
    "        porcentajes.append(parts[2])  # Porcentaje\n",
    "        if len(parts) > 3:\n",
    "            palabras_clave_temp.append(parts[3])  # Primera línea de palabras clave\n",
    "    else:\n",
    "        # Continuar acumulando palabras clave si no es un nuevo cluster\n",
    "        palabras_clave_temp.append(line.strip())\n",
    "\n",
    "# Agregar las últimas palabras clave acumuladas\n",
    "if palabras_clave_temp:\n",
    "    palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "# Verificar que todas las listas tengan la misma longitud\n",
    "if not (len(clusters) == len(cantidad_palabras) == len(porcentajes) == len(palabras_clave)):\n",
    "    print(f\"Longitudes inconsistentes: clusters={len(clusters)}, \"\n",
    "          f\"cantidad_palabras={len(cantidad_palabras)}, \"\n",
    "          f\"porcentajes={len(porcentajes)}, \"\n",
    "          f\"palabras_clave={len(palabras_clave)}\")\n",
    "    raise ValueError(\"Las listas no tienen la misma longitud.\")\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Cluster\": clusters,\n",
    "    \"Cantidad de palabras clave\": cantidad_palabras,\n",
    "    \"Porcentaje de palabras clave\": porcentajes,\n",
    "    \"Principales palabras clave\": palabras_clave\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df)\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV (opcional)\n",
    "df.to_csv(\"clusters_report.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 & 0 & 0 & 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de entrada\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "# Listas para almacenar los datos procesados\n",
    "clusters = []\n",
    "cantidad_palabras = []\n",
    "porcentajes = []\n",
    "palabras_clave = []\n",
    "\n",
    "# Leer y procesar el archivo\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Variables temporales\n",
    "palabras_clave_temp = []\n",
    "\n",
    "# Procesar cada línea del archivo\n",
    "for i, line in enumerate(lines):\n",
    "    # Omitir encabezados y separadores\n",
    "    if i < 4 or line.startswith('-'):\n",
    "        continue\n",
    "\n",
    "    # Detectar líneas que inician un nuevo cluster\n",
    "    try:\n",
    "        # Intentar convertir los primeros caracteres en un número\n",
    "        cluster_num = int(line[:3].strip())\n",
    "        # Si tenemos palabras clave acumuladas, guardarlas antes de continuar\n",
    "        if palabras_clave_temp:\n",
    "            palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "            palabras_clave_temp = []\n",
    "        \n",
    "        # Extraer datos del cluster actual\n",
    "        parts = line.split(maxsplit=3)\n",
    "        clusters.append(cluster_num)  # Número del cluster\n",
    "        cantidad_palabras.append(int(parts[1]))  # Cantidad de palabras clave\n",
    "        porcentajes.append(parts[2])  # Porcentaje\n",
    "        if len(parts) > 3:\n",
    "            palabras_clave_temp.append(parts[3])  # Primera línea de palabras clave\n",
    "    except ValueError:\n",
    "        # Si no es un número, acumular palabras clave\n",
    "        palabras_clave_temp.append(line.strip())\n",
    "\n",
    "# Agregar las últimas palabras clave acumuladas\n",
    "if palabras_clave_temp:\n",
    "    palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "print( str(len(clusters))+ \" & \"+ str(len(cantidad_palabras))+ \" & \"+ str(len(porcentajes))+ \" & \" +str(len(palabras_clave)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>cantidad_de_palabras_clave</th>\n",
       "      <th>porcentaje_de_palabras_clave</th>\n",
       "      <th>principales_palabras_clave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>15.9</td>\n",
       "      <td>maximum power point tracking, , fuzzy-logic ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>15.4</td>\n",
       "      <td>support vector machine, , long short-term memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>13.4</td>\n",
       "      <td>smart grid, , wind power, , reinforcement lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>9.1</td>\n",
       "      <td>wind turbine, , fault diagnosis, , biodiesel, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>7.9</td>\n",
       "      <td>electric vehicle, , lithium-ion batteries, , s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>7.7</td>\n",
       "      <td>particle swarm optimization, , distribute gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>6.3</td>\n",
       "      <td>multi-objective optimization, , energy storage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>5.7</td>\n",
       "      <td>genetic algorithm, , demand-side management, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>5.3</td>\n",
       "      <td>anfis, , global solar irradiance, , solar irra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>4.1</td>\n",
       "      <td>micro grid, , multi-agent systems, , distribut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>3.3</td>\n",
       "      <td>hydrogen, , biochar, , biomass, , biogas, , mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>3.3</td>\n",
       "      <td>state of charge (soc) estimation, , radial bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>2.6</td>\n",
       "      <td>pem fuel cell, , solid-oxide fuel cell, , deep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  cantidad_de_palabras_clave  porcentaje_de_palabras_clave  \\\n",
       "0         1                         105                          15.9   \n",
       "1         2                         102                          15.4   \n",
       "2         3                          89                          13.4   \n",
       "3         4                          60                           9.1   \n",
       "4         5                          52                           7.9   \n",
       "5         6                          51                           7.7   \n",
       "6         7                          42                           6.3   \n",
       "7         8                          38                           5.7   \n",
       "8         9                          35                           5.3   \n",
       "9        10                          27                           4.1   \n",
       "10       11                          22                           3.3   \n",
       "11       12                          22                           3.3   \n",
       "12       13                          17                           2.6   \n",
       "\n",
       "                           principales_palabras_clave  \n",
       "0   maximum power point tracking, , fuzzy-logic ba...  \n",
       "1   support vector machine, , long short-term memo...  \n",
       "2   smart grid, , wind power, , reinforcement lear...  \n",
       "3   wind turbine, , fault diagnosis, , biodiesel, ...  \n",
       "4   electric vehicle, , lithium-ion batteries, , s...  \n",
       "5   particle swarm optimization, , distribute gene...  \n",
       "6   multi-objective optimization, , energy storage...  \n",
       "7   genetic algorithm, , demand-side management, ,...  \n",
       "8   anfis, , global solar irradiance, , solar irra...  \n",
       "9   micro grid, , multi-agent systems, , distribut...  \n",
       "10  hydrogen, , biochar, , biomass, , biogas, , mi...  \n",
       "11  state of charge (soc) estimation, , radial bas...  \n",
       "12  pem fuel cell, , solid-oxide fuel cell, , deep...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo de entrada\n",
    "file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "# Listas para almacenar los datos procesados\n",
    "clusters = []\n",
    "cantidad_palabras = []\n",
    "porcentajes = []\n",
    "palabras_clave = []\n",
    "\n",
    "# Abrir y procesar el archivo\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Procesar cada línea del archivo\n",
    "palabras_clave_temp = []\n",
    "for i, line in enumerate(lines):\n",
    "    # Omitir encabezados y separadores\n",
    "    if i < 4 or line.startswith('-'):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Intentar convertir los primeros caracteres en un número\n",
    "        cluster_num = int(line.strip()[:3])\n",
    "        # Si tenemos palabras clave acumuladas, guardarlas antes de continuar\n",
    "        if palabras_clave_temp:\n",
    "            palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "            palabras_clave_temp = []\n",
    "        \n",
    "        # Extraer datos del cluster actual\n",
    "        parts = lines[i].split()\n",
    "        clusters.append(int(parts[0]))\n",
    "        cantidad_palabras.append(int(parts[1]))\n",
    "        porcentajes.append(float(parts[2].replace(\",\",\".\")))\n",
    "        palabras_clave_temp.append(\" \".join(parts[4:]))\n",
    "    except ValueError:\n",
    "        # Si no es un número, acumular palabras clave\n",
    "        palabras_clave_temp.append(line.strip())\n",
    "\n",
    "# Añadir las últimas palabras clave acumuladas\n",
    "if palabras_clave_temp:\n",
    "    palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "# Verificar que todas las listas tengan la misma longitud\n",
    "##assert len(clusters) == len(cantidad_palabras) == len(porcentajes) == len(palabras_clave), \\\n",
    "    ##\"Las listas no tienen la misma longitud.\"\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"cluster\": clusters,\n",
    "    \"cantidad_de_palabras_clave\": cantidad_palabras,\n",
    "    \"porcentaje_de_palabras_clave\": porcentajes,\n",
    "    \"principales_palabras_clave\": palabras_clave\n",
    "})\n",
    "def transformar_palabras_clave(palabras_clave):\n",
    "    # Reemplazar puntos y dividir por coma y espacio\n",
    "    palabras = palabras_clave.replace(\".\", \"\").replace(\"  \",\" \").split(\", \")\n",
    "    \n",
    "    # Crear una lista con coma y espacio en todos los elementos excepto el último\n",
    "    palabras_con_coma = [el.strip() + \", \" for el in palabras[:-1]] + [palabras[-1]]  # Último elemento sin coma\n",
    "    \n",
    "    # Convertir la lista en una tupla\n",
    "    return \"\".join((palabras_con_coma))\n",
    "\n",
    "# Aplicar la transformación a toda la columna\n",
    "df['principales_palabras_clave'] = df['principales_palabras_clave'].apply(transformar_palabras_clave)\n",
    "# Mostrar el DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('maximum power point tracking',\n",
       " 'fuzzy-logic based control',\n",
       " 'photo voltaic (pv)',\n",
       " 'photo-voltaic  system',\n",
       " ' differential   evolution   algorithm',\n",
       " '  evolutionary algorithm',\n",
       " 'double-fed induction generator (dfig)',\n",
       " 'ant  colony  optimisation',\n",
       " 'photo voltaic array',\n",
       " 'firefly algorithm',\n",
       " 'partial shade')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(df.principales_palabras_clave.to_list()[0].replace(\".\",\"\").split(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('maximum power point tracking, ',\n",
       " 'fuzzy-logic based control, ',\n",
       " 'photo voltaic (pv), ',\n",
       " 'photo-voltaic  system, ',\n",
       " ' differential   evolution   algorithm, ',\n",
       " '  evolutionary algorithm, ',\n",
       " 'double-fed induction generator (dfig), ',\n",
       " 'ant  colony  optimisation, ',\n",
       " 'photo voltaic array, ',\n",
       " 'firefly algorithm, ',\n",
       " 'partial shade, ')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(el + \", \" for el in tuple(df.principales_palabras_clave.to_list()[0].replace(\".\",\"\").split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     'maximum power point tracking, 'fuzzy-logic ba...\n",
       "1     'support vector machine, 'long short-term memo...\n",
       "2     'smart grid, 'wind power, 'reinforcement learn...\n",
       "3     'wind turbine, 'fault diagnosis, 'biodiesel, '...\n",
       "4     'electric vehicle, 'lithium-ion batteries, 'st...\n",
       "5     'particle swarm optimization, 'distribute gene...\n",
       "6     'multi-objective optimization, 'energy storage...\n",
       "7     'genetic algorithm, 'demand-side management, '...\n",
       "8     'anfis, 'global solar irradiance, 'solar irrad...\n",
       "9     'micro grid, 'multi-agent systems, 'distribute...\n",
       "10    'hydrogen, 'biochar, 'biomass, 'biogas, 'micro...\n",
       "11    'state of charge (soc) estimation, 'radial bas...\n",
       "12    'pem fuel cell, 'solid-oxide fuel cell, 'deep-...\n",
       "Name: principales_palabras_clave, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['principales_palabras_clave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.principales_palabras_clave.to_list()[0] == (\n",
    "        \"maximum power point tracking, \"\n",
    "        \"fuzzy-logic based control, \"\n",
    "        \"photo voltaic (pv), \"\n",
    "        \"photo-voltaic system, \"\n",
    "        \"differential evolution algorithm, \"\n",
    "        \"evolutionary algorithm, \"\n",
    "        \"double-fed induction generator (dfig), \"\n",
    "        \"ant colony optimisation, \"\n",
    "        \"photo voltaic array, \"\n",
    "        \"firefly algorithm, \"\n",
    "        \"partial shade\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( (\n",
    "        \"maximum power point tracking, \"\n",
    "        \"fuzzy-logic based control, \"\n",
    "        \"photo voltaic (pv), \"\n",
    "        \"photo-voltaic system, \"\n",
    "        \"differential evolution algorithm, \"\n",
    "        \"evolutionary algorithm, \"\n",
    "        \"double-fed induction generator (dfig), \"\n",
    "        \"ant colony optimisation, \"\n",
    "        \"photo voltaic array, \"\n",
    "        \"firefly algorithm, \"\n",
    "        \"partial shade\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum power point tracking, , fuzzy-logic based control, , photo voltaic (pv), , photo-voltaic system, , differential  evolution  algorithm, , evolutionary algorithm, , double-fed induction generator (dfig), , ant colony optimisation, , photo voltaic array, , firefly algorithm, , partial shade'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.principales_palabras_clave.to_list()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster  cantidad_de_palabras_clave  porcentaje_de_palabras_clave  \\\n",
      "0         1                         105                          15.9   \n",
      "1         2                         102                          15.4   \n",
      "2         3                          89                          13.4   \n",
      "3         4                          60                           9.1   \n",
      "4         5                          52                           7.9   \n",
      "5         6                          51                           7.7   \n",
      "6         7                          42                           6.3   \n",
      "7         8                          38                           5.7   \n",
      "8         9                          35                           5.3   \n",
      "9        10                          27                           4.1   \n",
      "10       11                          22                           3.3   \n",
      "11       12                          22                           3.3   \n",
      "12       13                          17                           2.6   \n",
      "\n",
      "                           principales_palabras_clave  \n",
      "0   (maximum power point tracking, , fuzzy-logic b...  \n",
      "1   (support vector machine, , long short-term mem...  \n",
      "2   (smart grid, , wind power, , reinforcement lea...  \n",
      "3   (wind turbine, , fault diagnosis, , biodiesel,...  \n",
      "4   (electric vehicle, , lithium-ion batteries, , ...  \n",
      "5   (particle swarm optimization, , distribute gen...  \n",
      "6   (multi-objective optimization, , energy storag...  \n",
      "7   (genetic algorithm, , demand-side management, ...  \n",
      "8   (anfis, , global solar irradiance, , solar irr...  \n",
      "9   (micro grid, , multi-agent systems, , distribu...  \n",
      "10  (hydrogen, , biochar, , biomass, , biogas, , m...  \n",
      "11  (state of charge (soc) estimation, , radial ba...  \n",
      "12  (pem fuel cell, , solid-oxide fuel cell, , dee...  \n"
     ]
    }
   ],
   "source": [
    "def pregunta_01():\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ruta al archivo de entrada\n",
    "    file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "    # Listas para almacenar los datos procesados\n",
    "    clusters = []\n",
    "    cantidad_palabras = []\n",
    "    porcentajes = []\n",
    "    palabras_clave = []\n",
    "\n",
    "    # Abrir y procesar el archivo\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Procesar cada línea del archivo\n",
    "    palabras_clave_temp = []\n",
    "    for i, line in enumerate(lines):\n",
    "        # Omitir encabezados y separadores\n",
    "        if i < 4 or line.startswith('-'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Intentar convertir los primeros caracteres en un número\n",
    "            cluster_num = int(line.strip()[:3])\n",
    "            # Si tenemos palabras clave acumuladas, guardarlas antes de continuar\n",
    "            if palabras_clave_temp:\n",
    "                palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "                palabras_clave_temp = []\n",
    "            \n",
    "            # Extraer datos del cluster actual\n",
    "            parts = lines[i].split()\n",
    "            clusters.append(int(parts[0]))\n",
    "            cantidad_palabras.append(int(parts[1]))\n",
    "            porcentajes.append(float(parts[2].replace(\",\",\".\")))\n",
    "            palabras_clave_temp.append(\" \".join(parts[4:]))\n",
    "        except ValueError:\n",
    "            # Si no es un número, acumular palabras clave\n",
    "            palabras_clave_temp.append(line.strip())\n",
    "\n",
    "    # Añadir las últimas palabras clave acumuladas\n",
    "    if palabras_clave_temp:\n",
    "        palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "    # Verificar que todas las listas tengan la misma longitud\n",
    "    ##assert len(clusters) == len(cantidad_palabras) == len(porcentajes) == len(palabras_clave), \\\n",
    "        ##\"Las listas no tienen la misma longitud.\"\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"cluster\": clusters,\n",
    "        \"cantidad_de_palabras_clave\": cantidad_palabras,\n",
    "        \"porcentaje_de_palabras_clave\": porcentajes,\n",
    "        \"principales_palabras_clave\": palabras_clave\n",
    "    })\n",
    "    def transformar_palabras_clave(palabras_clave):\n",
    "        # Reemplazar puntos y dividir por coma y espacio\n",
    "        palabras = palabras_clave.replace(\".\", \"\").replace(\"  \",\" \").split(\", \")\n",
    "        \n",
    "        # Crear una lista con coma y espacio en todos los elementos excepto el último\n",
    "        palabras_con_coma = [el.strip() + \", \" for el in palabras[:-1]] + [palabras[-1]]  # Último elemento sin coma\n",
    "        \n",
    "        # Convertir la lista en una tupla\n",
    "        return tuple((palabras_con_coma))\n",
    "\n",
    "    # Aplicar la transformación a toda la columna\n",
    "    df['principales_palabras_clave'] = df['principales_palabras_clave'].apply(transformar_palabras_clave)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster  cantidad_de_palabras_clave  porcentaje_de_palabras_clave  \\\n",
      "0         1                         105                          15.9   \n",
      "1         2                         102                          15.4   \n",
      "2         3                          89                          13.4   \n",
      "3         4                          60                           9.1   \n",
      "4         5                          52                           7.9   \n",
      "5         6                          51                           7.7   \n",
      "6         7                          42                           6.3   \n",
      "7         8                          38                           5.7   \n",
      "8         9                          35                           5.3   \n",
      "9        10                          27                           4.1   \n",
      "10       11                          22                           3.3   \n",
      "11       12                          22                           3.3   \n",
      "12       13                          17                           2.6   \n",
      "\n",
      "                           principales_palabras_clave  \n",
      "0   (maximum power point tracking, , fuzzy-logic b...  \n",
      "1   (support vector machine, , long short-term mem...  \n",
      "2   (smart grid, , wind power, , reinforcement lea...  \n",
      "3   (wind turbine, , fault diagnosis, , biodiesel,...  \n",
      "4   (electric vehicle, , lithium-ion batteries, , ...  \n",
      "5   (particle swarm optimization, , distribute gen...  \n",
      "6   (multi-objective optimization, , energy storag...  \n",
      "7   (genetic algorithm, , demand-side management, ...  \n",
      "8   (anfis, , global solar irradiance, , solar irr...  \n",
      "9   (micro grid, , multi-agent systems, , distribu...  \n",
      "10  (hydrogen, , biochar, , biomass, , biogas, , m...  \n",
      "11  (state of charge (soc) estimation, , radial ba...  \n",
      "12  (pem fuel cell, , solid-oxide fuel cell, , dee...  \n"
     ]
    }
   ],
   "source": [
    "def pregunta_01():\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ruta al archivo de entrada\n",
    "    file_path = \"../files/input/clusters_report.txt\"\n",
    "\n",
    "    # Listas para almacenar los datos procesados\n",
    "    clusters = []\n",
    "    cantidad_palabras = []\n",
    "    porcentajes = []\n",
    "    palabras_clave = []\n",
    "\n",
    "    # Abrir y procesar el archivo\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Procesar cada línea del archivo\n",
    "    palabras_clave_temp = []\n",
    "    for i, line in enumerate(lines):\n",
    "        # Omitir encabezados y separadores\n",
    "        if i < 4 or line.startswith('-'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Intentar convertir los primeros caracteres en un número\n",
    "            cluster_num = int(line.strip()[:3])\n",
    "            # Si tenemos palabras clave acumuladas, guardarlas antes de continuar\n",
    "            if palabras_clave_temp:\n",
    "                palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "                palabras_clave_temp = []\n",
    "            \n",
    "            # Extraer datos del cluster actual\n",
    "            parts = lines[i].split()\n",
    "            clusters.append(int(parts[0]))\n",
    "            cantidad_palabras.append(int(parts[1]))\n",
    "            porcentajes.append(float(parts[2].replace(\",\",\".\")))\n",
    "            palabras_clave_temp.append(\" \".join(parts[4:]))\n",
    "        except ValueError:\n",
    "            # Si no es un número, acumular palabras clave\n",
    "            palabras_clave_temp.append(line.strip())\n",
    "\n",
    "    # Añadir las últimas palabras clave acumuladas\n",
    "    if palabras_clave_temp:\n",
    "        palabras_clave.append(\" \".join(palabras_clave_temp).strip())\n",
    "\n",
    "    # Verificar que todas las listas tengan la misma longitud\n",
    "    ##assert len(clusters) == len(cantidad_palabras) == len(porcentajes) == len(palabras_clave), \\\n",
    "        ##\"Las listas no tienen la misma longitud.\"\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"cluster\": clusters,\n",
    "        \"cantidad_de_palabras_clave\": cantidad_palabras,\n",
    "        \"porcentaje_de_palabras_clave\": porcentajes,\n",
    "        \"principales_palabras_clave\": palabras_clave\n",
    "    })\n",
    "    def transformar_palabras_clave(palabras_clave):\n",
    "        # Reemplazar puntos y dividir por coma y espacio\n",
    "        palabras = palabras_clave.replace(\".\", \"\").replace(\"  \",\" \").split(\", \")\n",
    "        \n",
    "        # Crear una lista con coma y espacio en todos los elementos excepto el último\n",
    "        palabras_con_coma = [el.strip() + \", \" for el in palabras[:-1]] + [palabras[-1]]  # Último elemento sin coma\n",
    "        \n",
    "        # Convertir la lista en una tupla\n",
    "        return tuple((palabras_con_coma))\n",
    "\n",
    "    # Aplicar la transformación a toda la columna\n",
    "    df['principales_palabras_clave'] = df['principales_palabras_clave'].apply(transformar_palabras_clave)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'principales_palabras_clave'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17536\\3469834306.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprincipales_palabras_clave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\usuario\\OneDrive\\Escritorio\\Posgrado\\Descriptiva\\2024-2-PRE-03-ingestion-de-texto-plano-dzaptaca\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'principales_palabras_clave'"
     ]
    }
   ],
   "source": [
    "df.principales_palabras_clave.to_list()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
